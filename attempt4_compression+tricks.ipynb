{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIqmrnvr5swJ"
      },
      "source": [
        "## Dataset\n",
        "For this part of the assignment, you will be working with the CIFAR100 dataset (already loaded above). This dataset consists of 60K 32x32 color images from 100 classes, with 600 images per class. There are 50K training images and 10K test images. The images in CIFAR100 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels.\n",
        "## BaseNet\n",
        "We created a BaseNet that you can run and get a baseline accuracy\n",
        "\n",
        "## Goal\n",
        "Your goal is to edit the BaseNet class or make new classes for devising **a effective（accuracy & Floats & Params） deep net architecture** through what you have learned in this course\n",
        "\n",
        "## Submission\n",
        "**Before due**，submitting your work to **aleeyanger@163.com**\n",
        "Attention:\n",
        "YOUR FILE SHOULDE BE LIKE THIS\n",
        "\n",
        "```\n",
        "  FINAL_(YOUR_TREAM_NUMBER).zip:\n",
        "      --Report.pdf\n",
        "      --code.zip\n",
        "```\n",
        "  EXAMPLE:\n",
        "`  T1G1.zip`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyOL6SR9HiH"
      },
      "source": [
        "##BASELINE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JQBOzIkQ9GDO"
      },
      "outputs": [],
      "source": [
        "#Device check and load model into device\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbDxHuMdBJDv",
        "outputId": "3d303e13-6ac5-4ee7-d117-0a5ab84dae08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in c:\\programdata\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in c:\\programdata\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from thop) (1.11.0)\n",
            "Requirement already satisfied: typing_extensions in c:\\programdata\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from torch>=1.0.0->thop) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "#install thop for count PARAMS and Flops\n",
        "! pip install thop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "382MyuCi5d2B",
        "outputId": "3c96ec69-0018-46ad-e5c6-18fc043e9cba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "##HYPER-PARAM\n",
        "batch_size = 400\n",
        "epochs = 120\n",
        "max_lr = 0.001\n",
        "grad_clip = 0.01\n",
        "weight_decay =0.001\n",
        "#weight_decay =5e-4*batch_size\n",
        "opt_func = torch.optim.Adam\n",
        "\n",
        "##DOWNLOAD dataset\n",
        "train_data = torchvision.datasets.CIFAR100('./', train=True, download=True)\n",
        "# Stick all the images together to form a 1600000 X 32 X 3 array\n",
        "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
        "# calculate the mean and std along the (0, 1) axes\n",
        "mean = np.mean(x, axis=(0, 1))/255\n",
        "std = np.std(x, axis=(0, 1))/255\n",
        "# the the mean and std\n",
        "mean=mean.tolist()\n",
        "std=std.tolist()\n",
        "\n",
        "##TRANSFORM\n",
        "transform_train = tt.Compose([tt.RandomCrop(32, padding=4,padding_mode='reflect'), \n",
        "                         tt.RandomHorizontalFlip(), \n",
        "                         tt.ToTensor(), \n",
        "                         tt.Normalize(mean,std,inplace=True)])\n",
        "transform_test = tt.Compose([tt.ToTensor(), tt.Normalize(mean,std)])\n",
        "##DATASET and DATALOADER\n",
        "trainset = torchvision.datasets.CIFAR100(\"./\",\n",
        "                                         train=True,\n",
        "                                         download=True,\n",
        "                                         transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\"./\",\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size*2,pin_memory=True, num_workers=2)\n",
        "#LOADER\n",
        "device = get_default_device()\n",
        "trainloader = DeviceDataLoader(trainloader, device)\n",
        "testloader = DeviceDataLoader(testloader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jzrXsUuU9WTi"
      },
      "outputs": [],
      "source": [
        "##TRAINING SETUP\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        #labels=smooth_one_hot(labels)\n",
        "        out = self(images) \n",
        "        #print(out)                 # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        #labels=smooth_one_hot(labels)\n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2tPOLyG4plb_"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight=True, bias=True):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        self.weight.data.fill_(1.0)\n",
        "        self.bias.data.fill_(0.0)\n",
        "        self.weight.requires_grad = weight\n",
        "        self.bias.requires_grad = bias\n",
        "\n",
        "\n",
        "class GhostBatchNorm(BatchNorm):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features * self.num_splits))\n",
        "        self.register_buffer('running_var', torch.ones(num_features * self.num_splits))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if (self.training is True) and (mode is False):  # lazily collate stats when we are going to use them\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(\n",
        "                self.num_splits)\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(\n",
        "                self.num_splits)\n",
        "        return super().train(mode)\n",
        "\n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            return F.batch_norm(\n",
        "                input.view(-1, C * self.num_splits, H, W), self.running_mean, self.running_var,\n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W)\n",
        "        else:\n",
        "            return F.batch_norm(\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features],\n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qUa_95QRrvrC"
      },
      "outputs": [],
      "source": [
        "#NET\n",
        "dropout_value=0.3\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    alpha=0.3\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              GhostBatchNorm(out_channels,num_splits=16),\n",
        "              nn.Dropout(dropout_value),]\n",
        "              #nn.MaxPool2d(2), \n",
        "              #nn.CELU(alpha),]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    layers.append(nn.CELU(alpha))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class Ghost_ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True) \n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) \n",
        "        \n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True) \n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) \n",
        "        self.conv5 = conv_block(512, 1028, pool=True) \n",
        "        self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))  \n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
        "                                        nn.Flatten(), # 1028 \n",
        "                                        nn.Linear(1028, num_classes)) # 1028 -> 100\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.conv5(out)\n",
        "        out = self.res3(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "GNmodel = to_device(Ghost_ResNet9(3,100), device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gx59G_Ve9nqF"
      },
      "outputs": [],
      "source": [
        "#Training Setup\n",
        "@torch.no_grad()\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in test_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, test_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, test_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDa2vTlL9rRB",
        "outputId": "75ae1226-4f1f-4933-8f83-babdf28aea71"
      },
      "outputs": [],
      "source": [
        "#epochs=40\n",
        "#Training(Using Multi_LR)\n",
        "history = [evaluate(model, testloader)] ## Initial evaluation\n",
        "# Fitting the first 1/4 \n",
        "current_time=time.time()\n",
        "history += fit_one_cycle(int(epochs/4), max_lr, GNmodel, trainloader, testloader, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)\n",
        "# Fitting the first 2/4 epochs\n",
        "history += fit_one_cycle(int(epochs/4), max_lr/10, GNmodel, trainloader, testloader, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)\n",
        "# Fitting the first 3/4 \n",
        "history += fit_one_cycle(int(epochs/4), max_lr/100, GNmodel, trainloader, testloader, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)\n",
        "# Fitting the first 4/4 epochs\n",
        "history += fit_one_cycle(int(epochs/4), max_lr/100, GNmodel, trainloader, testloader, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)\n",
        "# Print training time\n",
        "time_train = time.time() - current_time\n",
        "print('Training time: {:.2f} s'.format(time_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "UgXS_hUU_046",
        "outputId": "34d96e43-18e4-4b5b-e944-0e66b66071cf"
      },
      "outputs": [],
      "source": [
        "# Collect training time and result\n",
        "current_time = time.time()\n",
        "result = evaluate(GNmodel, testloader)\n",
        "result\n",
        "time_inference = time.time() - current_time\n",
        "print('Inference time: {:.2f} s'.format(time_inference))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "RJoBnHx81IB-",
        "outputId": "6a6c382f-c41e-496c-9783-1293f8f61b74"
      },
      "outputs": [],
      "source": [
        "# Saving the model to h5 file\n",
        "Path='./drive/My Drive/GNmodel1.pth'\n",
        "Path1='./drive/My Drive/GNmodel1.h5'\n",
        "torch.save(GNmodel.state_dict(), Path)\n",
        "torch.save(GNmodel.state_dict(), Path1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS8bTaid-Tlk"
      },
      "outputs": [],
      "source": [
        "# Saving the model to h5 file\n",
        "Path='./drive/My Drive/GNmodel1.pth'\n",
        "Path1='./drive/My Drive/GNmodel1.h5'\n",
        "torch.save(GNmodel.state_dict(), Path)\n",
        "torch.save(GNmodel.state_dict(), Path1)\n",
        "# Generate testing accuracy, predicted label, confusion matrix, and table for classification report\n",
        "def test_label_predictions(model, device, test_loader):\n",
        "    model.eval()\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            prediction = output.argmax(dim=1, keepdim=True)\n",
        "            actuals.extend(target.view_as(prediction))\n",
        "            predictions.extend(prediction)\n",
        "    return [i.item() for i in actuals], [i.item() for i in predictions]\n",
        "\n",
        "y_test, y_pred = test_label_predictions(GNmodel, device, testloader)\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "cr=classification_report(y_test, y_pred)\n",
        "fs=f1_score(y_test,y_pred,average='weighted')\n",
        "rs=recall_score(y_test, y_pred,average='weighted')\n",
        "accuracy=accuracy_score(y_test, y_pred)\n",
        "print('Confusion matrix:')\n",
        "print(cm)\n",
        "print(cr)\n",
        "print('F1 score: %f' % fs)\n",
        "print('Recall score: %f' % rs)\n",
        "print('Accuracy score: %f' % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tKzKYrO_gzV"
      },
      "outputs": [],
      "source": [
        "#Train Time\n",
        "print('Training time: {:.2f} s'.format(time_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyZZyHVi_iiY"
      },
      "outputs": [],
      "source": [
        "#Inference Time (Test Time)\n",
        "print('Inference time: {:.2f} s'.format(time_inference))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2eMiKDrk_nG8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.GhostBatchNorm'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.CELU'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.flatten.Flatten'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Ghost_ResNet9'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "531641968.0\n",
            "30430880.0\n"
          ]
        }
      ],
      "source": [
        "#Paramater Size and FLOPS\n",
        "from thop import profile\n",
        " \n",
        "input = torch.randn(1,3,32,32)\n",
        "input = input.to(device)\n",
        "flops, params = profile(GNmodel, inputs=(input,))\n",
        "print(flops)\n",
        "print(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fevKTcLq_p76"
      },
      "outputs": [],
      "source": [
        "#Accuaray\n",
        "print('Accuracy score: %f' % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKnfsgE4-fj-"
      },
      "source": [
        "## **(Optional)Some Results you can use in your Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa7bOMB8-a1X"
      },
      "outputs": [],
      "source": [
        "# Plot classification report and save to pdf function\n",
        "def plot_classification(precision, recall, f1_score):\n",
        "    plt.rcParams['font.size'] = 12\n",
        "    plt.rc('axes', linewidth=1.75)\n",
        "    marker_size = 8\n",
        "    figsize = 6\n",
        "    plt.figure(figsize=(1.4 * figsize, figsize))\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(precision, 'o', markersize=marker_size)\n",
        "    plt.ylabel('Precision', fontsize=14)\n",
        "    plt.xticks([])\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(recall, 'o', markersize=marker_size)\n",
        "    plt.ylabel('Recall', fontsize=14)\n",
        "    plt.xticks([])\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(f1_score, 'o', markersize=marker_size)\n",
        "    plt.ylabel('F1-score', fontsize=14)\n",
        "    plt.xlabel('Class', fontsize=14)\n",
        "    plt.subplots_adjust(hspace=0.001)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"result.pdf\")\n",
        "# Plot classification report and save to pdf\n",
        "def plot_confusion_matrix(cm):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
        "    plt.colorbar()\n",
        "    plt.ylabel('True label', fontsize=14)\n",
        "    plt.xlabel('Predicted label', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"confusion_matrix.pdf\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FM-IQXb-0iw"
      },
      "outputs": [],
      "source": [
        "# Plot and save confusion matrix\n",
        "precision, recall, f1,_= precision_recall_fscore_support(y_test, y_pred)\n",
        "print(recall)\n",
        "plot_classification(precision, recall, f1)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=GNmodel\n",
        "from torch.quantization import get_default_qconfig\n",
        "import copy\n",
        "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "from torch.ao.quantization.fx.graph_module import ObservedGraphModule\n",
        "def quant_fx(model):\n",
        "    \"\"\"\n",
        "    使用Pytorch中的FX模式对模型进行量化\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    qconfig = get_default_qconfig(\"fbgemm\")  # 默认是静态量化\n",
        "    qconfig_dict = {\n",
        "        \"\": qconfig,\n",
        "        # 'object_type': []\n",
        "    }\n",
        "    model_to_quantize = copy.deepcopy(model)\n",
        "    prepared_model = prepare_fx(model_to_quantize, qconfig_dict)\n",
        "    print(\"prepared model: \", prepared_model)\n",
        "\n",
        "    quantized_model = convert_fx(prepared_model)\n",
        "    print(\"quantized model: \", quantized_model)\n",
        "    torch.save(model.state_dict(), \"r9.pth\")\n",
        "    torch.save(quantized_model.state_dict(), \"r9_quant.pth\")\n",
        "\n",
        "\n",
        "def calib_quant_model(model, calib_dataloader):\n",
        "    \"\"\"\n",
        "    校准函数\n",
        "    \"\"\"\n",
        "    assert isinstance(\n",
        "        model, ObservedGraphModule\n",
        "    ), \"model must be a perpared fx ObservedGraphModule.\"\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for inputs, labels in calib_dataloader:\n",
        "            model(inputs)\n",
        "    print(\"calib done.\")\n",
        "\n",
        "\n",
        "def quant_calib_and_eval(model):\n",
        "    # test only on CPU\n",
        "    model.to(torch.device(\"cpu\"))\n",
        "    model.eval()\n",
        "\n",
        "    qconfig = get_default_qconfig(\"fbgemm\")\n",
        "    qconfig_dict = {\n",
        "        \"\": qconfig,\n",
        "        # 'object_type': []\n",
        "    }\n",
        "\n",
        "    model2 = copy.deepcopy(model)\n",
        "    model_prepared = prepare_fx(model2, qconfig_dict)\n",
        "    model_int8 = convert_fx(model_prepared)\n",
        "    model_int8.load_state_dict(torch.load(\"r9_quant.pth\"))\n",
        "    model_int8.eval()\n",
        "\n",
        "    a = torch.randn([1, 3, 224, 224])\n",
        "    o1 = model(a)\n",
        "    o2 = model_int8(a)\n",
        "\n",
        "    diff = torch.allclose(o1, o2, 1e-4)\n",
        "    print(diff)\n",
        "    print(o1.shape, o2.shape)\n",
        "    print(o1, o2)\n",
        "    # get_output_from_logits(o1)\n",
        "    # get_output_from_logits(o2)\n",
        "\n",
        "    #train_loader, test_loader = prepare_dataloader()\n",
        "    print(\"model:\")\n",
        "    y_test, y_pred =test_label_predictions(model, device, testloader)\n",
        "    fs=f1_score(y_test,y_pred,average='weighted')\n",
        "    rs=recall_score(y_test, y_pred,average='weighted')\n",
        "    accuracy=accuracy_score(y_test, y_pred)\n",
        "    print('F1 score: %f' % fs)\n",
        "    print('Recall score: %f' % rs)\n",
        "    print('Accuracy score: %f' % accuracy)\n",
        "    print()\n",
        "    \n",
        "    print(\"Not calibration model_int8:\")\n",
        "    y_test, y_pred =test_label_predictions(model_int8, device, testloader)\n",
        "    fs=f1_score(y_test,y_pred,average='weighted')\n",
        "    rs=recall_score(y_test, y_pred,average='weighted')\n",
        "    accuracy=accuracy_score(y_test, y_pred)\n",
        "    print('F1 score: %f' % fs)\n",
        "    print('Recall score: %f' % rs)\n",
        "    print('Accuracy score: %f' % accuracy)\n",
        "    print()\n",
        "    \n",
        "    # calib quant model\n",
        "    model2 = copy.deepcopy(model)\n",
        "    model_prepared = prepare_fx(model2, qconfig_dict)\n",
        "    model_int8 = convert_fx(model_prepared)\n",
        "    torch.save(model_int8.state_dict(), \"r9.pth\")\n",
        "    model_int8.eval()\n",
        "\n",
        "    #model_prepared = prepare_fx(model2, qconfig_dict)\n",
        "    calib_quant_model(model2, testloader)  # 对模型进行校准\n",
        "    model_int8 = convert_fx(model2)\n",
        "    torch.save(model_int8.state_dict(), \"r9_quant_calib.pth\")\n",
        "    print(\"Do calibration model_int8:\")\n",
        "    evaluate_model(model_int8, testloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "if os.path.exists(\"r9_row.pth\"):\n",
        "        model.load_state_dict(torch.load(\"r9_row.pth\", map_location=\"cpu\"))\n",
        "# else:\n",
        "#         train_model(model, train_loader, test_loader, torch.device(\"cuda\"))\n",
        "#         print(\"train finished.\")\n",
        "#         torch.save(model.state_dict(), \"r18_row.pth\")\n",
        "    # 模型量化\n",
        "quant_fx(model)\n",
        "    # 对比是否 calibration 的影响\n",
        "quant_calib_and_eval(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('pytorch_gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "112afcdae7d1037ee1f4274e23bc7ac65922657208ceff315e859399619f393c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
