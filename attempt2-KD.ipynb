{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ee87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device check and load model into device\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c855fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "##HYPER-PARAM\n",
    "batch_size =16\n",
    "epochs = 120\n",
    "max_lr = 0.001\n",
    "grad_clip = 0.01\n",
    "weight_decay =0.001\n",
    "opt_func = torch.optim.Adam\n",
    "\n",
    "##DOWNLOAD dataset\n",
    "train_data = torchvision.datasets.CIFAR100('./', train=True, download=True)\n",
    "# Stick all the images together to form a 1600000 X 32 X 3 array\n",
    "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
    "# calculate the mean and std along the (0, 1) axes\n",
    "mean = np.mean(x, axis=(0, 1))/255\n",
    "std = np.std(x, axis=(0, 1))/255\n",
    "# the the mean and std\n",
    "mean=mean.tolist()\n",
    "std=std.tolist()\n",
    "\n",
    "##TRANSFORM\n",
    "transform_train = tt.Compose([tt.RandomCrop(32, padding=4,padding_mode='reflect'), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(mean,std,inplace=True)])\n",
    "transform_test = tt.Compose([tt.ToTensor(), tt.Normalize(mean,std)])\n",
    "##DATASET and DATALOADER\n",
    "trainset = torchvision.datasets.CIFAR100(\"./\",\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\"./\",\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size*2,pin_memory=True, num_workers=2)\n",
    "#LOADER\n",
    "device = get_default_device()\n",
    "trainloader = DeviceDataLoader(trainloader, device)\n",
    "testloader = DeviceDataLoader(testloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4d1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.models.resnet import resnet9\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.datasets as dst\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "resnet50_pretrain_weight = \"teacher_model.pth\"\n",
    "resnet18_pretrain_weight = \"student_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68bcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c35ea86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True) \n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) \n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True) \n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) \n",
    "        self.conv5 = conv_block(512, 1028, pool=True) \n",
    "        self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))  \n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
    "                                        nn.Flatten(), # 1028 \n",
    "                                        nn.Linear(1028, num_classes)) # 1028 -> 100\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.conv5(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f22d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet9_2(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 8)\n",
    "        self.conv2 = conv_block(8, 8, pool=True) \n",
    "        self.res1 = nn.Sequential(conv_block(8, 8), conv_block(8, 8)) \n",
    "        \n",
    "        self.conv3 = conv_block(8, 16, pool=True)\n",
    "        self.conv4 = conv_block(16, 32, pool=True) \n",
    "        self.res2 = nn.Sequential(conv_block(32, 32), conv_block(32, 32)) \n",
    "        self.conv5 = conv_block(32, 128, pool=True) \n",
    "        self.res3 = nn.Sequential(conv_block(128,128), conv_block(128, 128))  \n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
    "                                        nn.Flatten(), # 1028 \n",
    "                                        nn.Linear(128, num_classes)) # 1028 -> 100\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.conv5(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet9_3(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_block(in_channels, 8)\n",
    "        self.conv2 = conv_block(8, 8, pool=True) \n",
    "        self.res1 = nn.Sequential(conv_block(8, 8), conv_block(8, 8))  \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
    "                                        nn.Flatten(), # 1028 \n",
    "                                        nn.Linear(8, num_classes)) # 1028 -> 100\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8de4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss_init: 1.574031114578247, loss_kd: 5.030104637145996, loss_all:6.604135513305664\n",
      "Acc_student:0.46089285612106323, Acc_teacher: 0.7384821772575378\n",
      "epoch:1, loss_init: 1.6289645433425903, loss_kd: 4.643418312072754, loss_all:6.272382736206055\n",
      "Acc_student:0.45750004053115845, Acc_teacher: 0.7378571629524231\n",
      "epoch:2, loss_init: 1.6621828079223633, loss_kd: 4.825350284576416, loss_all:6.487533092498779\n",
      "Acc_student:0.4616071581840515, Acc_teacher: 0.7383928298950195\n",
      "epoch:3, loss_init: 1.517826795578003, loss_kd: 4.611663818359375, loss_all:6.129490852355957\n",
      "Acc_student:0.4641071557998657, Acc_teacher: 0.7397322058677673\n",
      "epoch:4, loss_init: 1.6665574312210083, loss_kd: 4.572325229644775, loss_all:6.238882541656494\n",
      "Acc_student:0.46223217248916626, Acc_teacher: 0.7382143139839172\n",
      "epoch:5, loss_init: 1.5219961404800415, loss_kd: 4.449599742889404, loss_all:5.971595764160156\n",
      "Acc_student:0.4613392949104309, Acc_teacher: 0.7377678751945496\n",
      "epoch:6, loss_init: 1.5658502578735352, loss_kd: 4.1968793869018555, loss_all:5.762729644775391\n",
      "Acc_student:0.4650000333786011, Acc_teacher: 0.7383036017417908\n",
      "epoch:7, loss_init: 1.5427442789077759, loss_kd: 4.218594074249268, loss_all:5.761338233947754\n",
      "Acc_student:0.465446412563324, Acc_teacher: 0.7379464507102966\n",
      "epoch:8, loss_init: 1.5405980348587036, loss_kd: 4.243775367736816, loss_all:5.7843732833862305\n",
      "Acc_student:0.466785728931427, Acc_teacher: 0.7391071915626526\n",
      "epoch:9, loss_init: 1.5338351726531982, loss_kd: 4.127593517303467, loss_all:5.661428451538086\n",
      "Acc_student:0.46491071581840515, Acc_teacher: 0.7380357384681702\n",
      "epoch:10, loss_init: 1.5168004035949707, loss_kd: 4.195085048675537, loss_all:5.711885452270508\n",
      "Acc_student:0.46866071224212646, Acc_teacher: 0.7391964793205261\n",
      "epoch:11, loss_init: 1.6875085830688477, loss_kd: 4.431023597717285, loss_all:6.118532180786133\n",
      "Acc_student:0.46616071462631226, Acc_teacher: 0.7378571629524231\n",
      "epoch:12, loss_init: 1.657597541809082, loss_kd: 4.129523754119873, loss_all:5.787121295928955\n",
      "Acc_student:0.4694643020629883, Acc_teacher: 0.7413393259048462\n",
      "epoch:13, loss_init: 1.457515001296997, loss_kd: 4.411293983459473, loss_all:5.868808746337891\n",
      "Acc_student:0.47062501311302185, Acc_teacher: 0.7424107193946838\n",
      "epoch:14, loss_init: 1.4307496547698975, loss_kd: 4.198550701141357, loss_all:5.629300117492676\n",
      "Acc_student:0.46714285016059875, Acc_teacher: 0.7408928275108337\n",
      "epoch:15, loss_init: 1.426873803138733, loss_kd: 3.9580845832824707, loss_all:5.384958267211914\n",
      "Acc_student:0.4683035910129547, Acc_teacher: 0.738839328289032\n",
      "epoch:16, loss_init: 1.5296244621276855, loss_kd: 4.233404636383057, loss_all:5.763029098510742\n",
      "Acc_student:0.470982164144516, Acc_teacher: 0.7400892972946167\n",
      "epoch:17, loss_init: 1.4285980463027954, loss_kd: 4.204570293426514, loss_all:5.6331682205200195\n",
      "Acc_student:0.47107142210006714, Acc_teacher: 0.7407143115997314\n",
      "epoch:18, loss_init: 1.649524211883545, loss_kd: 4.2762322425842285, loss_all:5.925756454467773\n",
      "Acc_student:0.4749107360839844, Acc_teacher: 0.738839328289032\n",
      "epoch:19, loss_init: 1.4163419008255005, loss_kd: 4.044968605041504, loss_all:5.461310386657715\n",
      "Acc_student:0.47008928656578064, Acc_teacher: 0.7348214387893677\n",
      "epoch:20, loss_init: 1.5521527528762817, loss_kd: 4.036903381347656, loss_all:5.589056015014648\n",
      "Acc_student:0.4746428430080414, Acc_teacher: 0.7425000071525574\n",
      "epoch:21, loss_init: 1.5694212913513184, loss_kd: 4.187114238739014, loss_all:5.756535530090332\n",
      "Acc_student:0.4715178608894348, Acc_teacher: 0.7413393259048462\n",
      "epoch:22, loss_init: 1.3708431720733643, loss_kd: 4.0203375816345215, loss_all:5.391180992126465\n",
      "Acc_student:0.4746428430080414, Acc_teacher: 0.7445535659790039\n",
      "epoch:23, loss_init: 1.5952019691467285, loss_kd: 4.239447116851807, loss_all:5.834649085998535\n",
      "Acc_student:0.475178599357605, Acc_teacher: 0.7433035969734192\n",
      "epoch:24, loss_init: 1.5059394836425781, loss_kd: 4.007311820983887, loss_all:5.513251304626465\n",
      "Acc_student:0.4747321605682373, Acc_teacher: 0.742767870426178\n",
      "epoch:25, loss_init: 1.443980097770691, loss_kd: 3.9106109142303467, loss_all:5.354590892791748\n",
      "Acc_student:0.4746428430080414, Acc_teacher: 0.7411606907844543\n",
      "epoch:26, loss_init: 1.4443286657333374, loss_kd: 3.868400812149048, loss_all:5.312729358673096\n",
      "Acc_student:0.47035714983940125, Acc_teacher: 0.7417857050895691\n",
      "epoch:27, loss_init: 1.5304484367370605, loss_kd: 3.9687318801879883, loss_all:5.499180316925049\n",
      "Acc_student:0.47383928298950195, Acc_teacher: 0.7416964769363403\n",
      "epoch:28, loss_init: 1.2393579483032227, loss_kd: 3.9557511806488037, loss_all:5.1951093673706055\n",
      "Acc_student:0.4750000238418579, Acc_teacher: 0.7447322010993958\n",
      "epoch:29, loss_init: 1.4417388439178467, loss_kd: 4.2144999504089355, loss_all:5.656238555908203\n",
      "Acc_student:0.4757142961025238, Acc_teacher: 0.7450893521308899\n",
      "epoch:30, loss_init: 1.3618659973144531, loss_kd: 3.897941827774048, loss_all:5.259807586669922\n",
      "Acc_student:0.4715178608894348, Acc_teacher: 0.7419643402099609\n",
      "epoch:31, loss_init: 1.5430527925491333, loss_kd: 3.945173978805542, loss_all:5.488226890563965\n",
      "Acc_student:0.4786607325077057, Acc_teacher: 0.7439286112785339\n",
      "epoch:32, loss_init: 1.4554747343063354, loss_kd: 4.0308451652526855, loss_all:5.4863200187683105\n",
      "Acc_student:0.47562503814697266, Acc_teacher: 0.7447322010993958\n",
      "epoch:33, loss_init: 1.498014211654663, loss_kd: 4.194246292114258, loss_all:5.6922607421875\n",
      "Acc_student:0.47366073727607727, Acc_teacher: 0.742767870426178\n",
      "epoch:34, loss_init: 1.389026165008545, loss_kd: 4.2091169357299805, loss_all:5.598143100738525\n",
      "Acc_student:0.4766964316368103, Acc_teacher: 0.746071457862854\n",
      "epoch:35, loss_init: 1.3229663372039795, loss_kd: 3.9183812141418457, loss_all:5.241347312927246\n",
      "Acc_student:0.47785717248916626, Acc_teacher: 0.7440178394317627\n",
      "epoch:36, loss_init: 1.4367526769638062, loss_kd: 3.9543569087982178, loss_all:5.391109466552734\n",
      "Acc_student:0.4794642925262451, Acc_teacher: 0.7469643354415894\n",
      "epoch:37, loss_init: 1.4196538925170898, loss_kd: 4.0861992835998535, loss_all:5.505853176116943\n",
      "Acc_student:0.4766964316368103, Acc_teacher: 0.7440178394317627\n",
      "epoch:38, loss_init: 1.4549082517623901, loss_kd: 3.999559164047241, loss_all:5.454467296600342\n",
      "Acc_student:0.47732141613960266, Acc_teacher: 0.7470535635948181\n",
      "epoch:39, loss_init: 1.459895133972168, loss_kd: 3.952498197555542, loss_all:5.412393569946289\n",
      "Acc_student:0.477142870426178, Acc_teacher: 0.7459821701049805\n",
      "epoch:40, loss_init: 1.4971193075180054, loss_kd: 4.033831596374512, loss_all:5.530951023101807\n",
      "Acc_student:0.478214293718338, Acc_teacher: 0.7444643378257751\n",
      "epoch:41, loss_init: 1.4524941444396973, loss_kd: 3.940768003463745, loss_all:5.393261909484863\n",
      "Acc_student:0.4800892770290375, Acc_teacher: 0.7446428537368774\n",
      "epoch:42, loss_init: 1.383484959602356, loss_kd: 3.855126142501831, loss_all:5.238611221313477\n",
      "Acc_student:0.4836607277393341, Acc_teacher: 0.7475000023841858\n",
      "epoch:43, loss_init: 1.3825209140777588, loss_kd: 3.9262444972991943, loss_all:5.308765411376953\n",
      "Acc_student:0.4776785969734192, Acc_teacher: 0.7489286065101624\n",
      "epoch:44, loss_init: 1.3167731761932373, loss_kd: 3.827147960662842, loss_all:5.1439208984375\n",
      "Acc_student:0.47812503576278687, Acc_teacher: 0.7463393211364746\n",
      "epoch:45, loss_init: 1.440253496170044, loss_kd: 3.802321672439575, loss_all:5.242575168609619\n",
      "Acc_student:0.4754464328289032, Acc_teacher: 0.7432143092155457\n",
      "epoch:46, loss_init: 1.4640648365020752, loss_kd: 3.7795708179473877, loss_all:5.243635654449463\n",
      "Acc_student:0.4754464328289032, Acc_teacher: 0.7445535659790039\n",
      "epoch:47, loss_init: 1.3382463455200195, loss_kd: 3.871452569961548, loss_all:5.209698677062988\n",
      "Acc_student:0.4809821546077728, Acc_teacher: 0.7457143068313599\n",
      "epoch:48, loss_init: 1.4142062664031982, loss_kd: 3.6638505458831787, loss_all:5.078056812286377\n",
      "Acc_student:0.47937503457069397, Acc_teacher: 0.7450893521308899\n",
      "epoch:49, loss_init: 1.3926804065704346, loss_kd: 3.8746886253356934, loss_all:5.267369270324707\n",
      "Acc_student:0.4783928692340851, Acc_teacher: 0.7456249594688416\n",
      "epoch:50, loss_init: 1.4515161514282227, loss_kd: 3.8407750129699707, loss_all:5.292291164398193\n",
      "Acc_student:0.47776785492897034, Acc_teacher: 0.7464286088943481\n",
      "epoch:51, loss_init: 1.4090975522994995, loss_kd: 3.8695147037506104, loss_all:5.27861213684082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc_student:0.4794642925262451, Acc_teacher: 0.7466964721679688\n",
      "epoch:52, loss_init: 1.2838106155395508, loss_kd: 3.6270580291748047, loss_all:4.9108686447143555\n",
      "Acc_student:0.4853571653366089, Acc_teacher: 0.7477678656578064\n",
      "epoch:53, loss_init: 1.5162208080291748, loss_kd: 4.031732559204102, loss_all:5.5479536056518555\n",
      "Acc_student:0.4784821569919586, Acc_teacher: 0.7475000023841858\n",
      "epoch:54, loss_init: 1.4713636636734009, loss_kd: 3.7979698181152344, loss_all:5.269333362579346\n",
      "Acc_student:0.4843750298023224, Acc_teacher: 0.7495535612106323\n",
      "epoch:55, loss_init: 1.3179320096969604, loss_kd: 3.624600410461426, loss_all:4.942532539367676\n",
      "Acc_student:0.47830358147621155, Acc_teacher: 0.7461607456207275\n",
      "epoch:56, loss_init: 1.2564806938171387, loss_kd: 3.6080408096313477, loss_all:4.864521503448486\n",
      "Acc_student:0.4794642925262451, Acc_teacher: 0.7446428537368774\n",
      "epoch:57, loss_init: 1.2513408660888672, loss_kd: 3.8414080142974854, loss_all:5.092748641967773\n",
      "Acc_student:0.4806250333786011, Acc_teacher: 0.7483035922050476\n",
      "epoch:58, loss_init: 1.2430741786956787, loss_kd: 3.867213249206543, loss_all:5.110287666320801\n",
      "Acc_student:0.48321428894996643, Acc_teacher: 0.74723219871521\n",
      "epoch:59, loss_init: 1.4913489818572998, loss_kd: 3.8610446453094482, loss_all:5.352393627166748\n",
      "Acc_student:0.4814285933971405, Acc_teacher: 0.7489286065101624\n",
      "epoch:60, loss_init: 1.3688873052597046, loss_kd: 3.69051194190979, loss_all:5.059399127960205\n",
      "Acc_student:0.47901785373687744, Acc_teacher: 0.74723219871521\n",
      "epoch:61, loss_init: 1.3222934007644653, loss_kd: 3.755779266357422, loss_all:5.078072547912598\n",
      "Acc_student:0.4825893044471741, Acc_teacher: 0.7466071844100952\n",
      "epoch:62, loss_init: 1.3774993419647217, loss_kd: 3.778179883956909, loss_all:5.155679225921631\n",
      "Acc_student:0.47910717129707336, Acc_teacher: 0.74723219871521\n",
      "epoch:63, loss_init: 1.3393967151641846, loss_kd: 3.775790214538574, loss_all:5.11518669128418\n",
      "Acc_student:0.48178571462631226, Acc_teacher: 0.7488393187522888\n",
      "epoch:64, loss_init: 1.3397114276885986, loss_kd: 3.6958978176116943, loss_all:5.035609245300293\n",
      "Acc_student:0.4841071665287018, Acc_teacher: 0.7484821677207947\n",
      "epoch:65, loss_init: 1.282837986946106, loss_kd: 3.8634612560272217, loss_all:5.146299362182617\n",
      "Acc_student:0.4863393008708954, Acc_teacher: 0.75\n",
      "epoch:66, loss_init: 1.2703256607055664, loss_kd: 3.6981844902038574, loss_all:4.968510150909424\n",
      "Acc_student:0.48000001907348633, Acc_teacher: 0.7455357313156128\n",
      "epoch:67, loss_init: 1.2602014541625977, loss_kd: 3.6407573223114014, loss_all:4.900959014892578\n",
      "Acc_student:0.4804464280605316, Acc_teacher: 0.7466071844100952\n",
      "epoch:68, loss_init: 1.193250298500061, loss_kd: 3.6178014278411865, loss_all:4.811051845550537\n",
      "Acc_student:0.48803573846817017, Acc_teacher: 0.7478572130203247\n",
      "epoch:69, loss_init: 1.2432383298873901, loss_kd: 3.4195234775543213, loss_all:4.662761688232422\n",
      "Acc_student:0.48625001311302185, Acc_teacher: 0.7505357265472412\n",
      "epoch:70, loss_init: 1.2712881565093994, loss_kd: 3.6080269813537598, loss_all:4.879315376281738\n",
      "Acc_student:0.48544642329216003, Acc_teacher: 0.7492856979370117\n",
      "epoch:71, loss_init: 1.3658485412597656, loss_kd: 3.7130253314971924, loss_all:5.078873634338379\n",
      "Acc_student:0.48678573966026306, Acc_teacher: 0.7474107146263123\n",
      "epoch:72, loss_init: 1.3636600971221924, loss_kd: 3.744365692138672, loss_all:5.108025550842285\n",
      "Acc_student:0.4839285910129547, Acc_teacher: 0.746874988079071\n",
      "epoch:73, loss_init: 1.3472567796707153, loss_kd: 3.8151750564575195, loss_all:5.162431716918945\n",
      "Acc_student:0.4890178442001343, Acc_teacher: 0.7479464411735535\n",
      "epoch:74, loss_init: 1.2703012228012085, loss_kd: 3.5122106075286865, loss_all:4.7825117111206055\n",
      "Acc_student:0.486607164144516, Acc_teacher: 0.7487499713897705\n",
      "epoch:75, loss_init: 1.3674054145812988, loss_kd: 3.555436372756958, loss_all:4.922842025756836\n",
      "Acc_student:0.4873214364051819, Acc_teacher: 0.7466071844100952\n",
      "epoch:76, loss_init: 1.2935528755187988, loss_kd: 3.8153555393218994, loss_all:5.108908653259277\n",
      "Acc_student:0.48669642210006714, Acc_teacher: 0.74723219871521\n",
      "epoch:77, loss_init: 1.192958116531372, loss_kd: 3.6140010356903076, loss_all:4.80695915222168\n",
      "Acc_student:0.48625001311302185, Acc_teacher: 0.7450000047683716\n",
      "epoch:78, loss_init: 1.2663224935531616, loss_kd: 3.787909984588623, loss_all:5.054232597351074\n",
      "Acc_student:0.49017858505249023, Acc_teacher: 0.7497321963310242\n",
      "epoch:79, loss_init: 1.3298072814941406, loss_kd: 3.9017598628997803, loss_all:5.2315673828125\n",
      "Acc_student:0.48830360174179077, Acc_teacher: 0.7490178346633911\n",
      "epoch:80, loss_init: 1.2517075538635254, loss_kd: 3.5251708030700684, loss_all:4.776878356933594\n",
      "Acc_student:0.4835714101791382, Acc_teacher: 0.7515178322792053\n",
      "epoch:81, loss_init: 1.2743198871612549, loss_kd: 3.635537624359131, loss_all:4.909857749938965\n",
      "Acc_student:0.48544642329216003, Acc_teacher: 0.748035728931427\n",
      "epoch:82, loss_init: 1.277315616607666, loss_kd: 3.574305295944214, loss_all:4.851620674133301\n",
      "Acc_student:0.4851785898208618, Acc_teacher: 0.7475000023841858\n",
      "epoch:83, loss_init: 1.2224187850952148, loss_kd: 3.6623528003692627, loss_all:4.884771347045898\n",
      "Acc_student:0.48803573846817017, Acc_teacher: 0.7496428489685059\n",
      "epoch:84, loss_init: 1.3207851648330688, loss_kd: 3.5988197326660156, loss_all:4.919604778289795\n",
      "Acc_student:0.4843750298023224, Acc_teacher: 0.7487499713897705\n",
      "epoch:85, loss_init: 1.2183533906936646, loss_kd: 3.7345471382141113, loss_all:4.952900409698486\n",
      "Acc_student:0.4861607253551483, Acc_teacher: 0.7473214268684387\n",
      "epoch:86, loss_init: 1.3226571083068848, loss_kd: 3.864574432373047, loss_all:5.187231540679932\n",
      "Acc_student:0.48723214864730835, Acc_teacher: 0.7501785755157471\n",
      "epoch:87, loss_init: 1.2784043550491333, loss_kd: 3.7629189491271973, loss_all:5.041323184967041\n",
      "Acc_student:0.4863393008708954, Acc_teacher: 0.7486607432365417\n",
      "epoch:88, loss_init: 1.302966594696045, loss_kd: 3.720581531524658, loss_all:5.023548126220703\n",
      "Acc_student:0.4883928596973419, Acc_teacher: 0.746071457862854\n",
      "epoch:89, loss_init: 1.3995388746261597, loss_kd: 3.823256731033325, loss_all:5.222795486450195\n",
      "Acc_student:0.4891071617603302, Acc_teacher: 0.7504464387893677\n",
      "epoch:90, loss_init: 1.1161670684814453, loss_kd: 3.543114185333252, loss_all:4.659281253814697\n",
      "Acc_student:0.4902678430080414, Acc_teacher: 0.749196469783783\n",
      "epoch:91, loss_init: 1.1260560750961304, loss_kd: 3.6281988620758057, loss_all:4.7542548179626465\n",
      "Acc_student:0.48794642090797424, Acc_teacher: 0.7499107122421265\n",
      "epoch:92, loss_init: 1.1343014240264893, loss_kd: 3.5648021697998047, loss_all:4.699103355407715\n",
      "Acc_student:0.48678573966026306, Acc_teacher: 0.7483035922050476\n",
      "epoch:93, loss_init: 1.2117319107055664, loss_kd: 3.676401376724243, loss_all:4.8881330490112305\n",
      "Acc_student:0.48446428775787354, Acc_teacher: 0.7500893473625183\n",
      "epoch:94, loss_init: 1.2875550985336304, loss_kd: 3.488093376159668, loss_all:4.775648593902588\n",
      "Acc_student:0.4890178442001343, Acc_teacher: 0.7481249570846558\n",
      "epoch:95, loss_init: 1.1630231142044067, loss_kd: 3.6768977642059326, loss_all:4.839920997619629\n",
      "Acc_student:0.490803599357605, Acc_teacher: 0.7529464364051819\n",
      "epoch:96, loss_init: 1.1499125957489014, loss_kd: 3.58186936378479, loss_all:4.731781959533691\n",
      "Acc_student:0.48928573727607727, Acc_teacher: 0.7508928775787354\n",
      "epoch:97, loss_init: 1.2179919481277466, loss_kd: 3.561790943145752, loss_all:4.779782772064209\n",
      "Acc_student:0.48794642090797424, Acc_teacher: 0.7466964721679688\n",
      "epoch:98, loss_init: 1.3506989479064941, loss_kd: 3.626777172088623, loss_all:4.977476119995117\n",
      "Acc_student:0.48151785135269165, Acc_teacher: 0.746071457862854\n",
      "epoch:99, loss_init: 1.21575129032135, loss_kd: 3.719182252883911, loss_all:4.934933662414551\n",
      "Acc_student:0.48928573727607727, Acc_teacher: 0.7500893473625183\n",
      "Training time: 6838.13 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "img_dir = \"/data/cifar10/\"\n",
    "\n",
    "\n",
    "def create_data(img_dir):\n",
    "    dataset = dst.CIFAR100\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2470, 0.2435, 0.2616)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.CenterCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    # define data loader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset(root=img_dir,\n",
    "                transform=train_transform,\n",
    "                train=True,\n",
    "                download=True),\n",
    "        batch_size=512, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset(root=img_dir,\n",
    "                transform=test_transform,\n",
    "                train=False,\n",
    "                download=True),\n",
    "        batch_size=512, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "'''\n",
    "\n",
    "def load_checkpoint(net, pth_file, exclude_fc=False):\n",
    "    if exclude_fc:\n",
    "        model_dict = net.state_dict()\n",
    "        pretrain_dict = torch.load(pth_file)\n",
    "        new_dict = {k: v for k, v in pretrain_dict.items() if 'fc' not in k}\n",
    "        model_dict.update(new_dict)\n",
    "        net.load_state_dict(model_dict, strict=False)\n",
    "    else:\n",
    "        pretrain_dict = torch.load(pth_file)\n",
    "        net.load_state_dict(pretrain_dict, strict=False)\n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    #res = res.contiguous().view(res.size()[0], -1)  #解决报错\n",
    "    for k in topk:\n",
    "        \n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
    "        \n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class KD_loss(nn.Module):\n",
    "    def __init__(self, T):\n",
    "        super(KD_loss, self).__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, out_s, out_t):\n",
    "        loss = F.kl_div(F.log_softmax(out_s / self.T, dim=1),\n",
    "                        F.softmax(out_t / self.T, dim=1),\n",
    "                        reduction='batchmean') * self.T * self.T\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def test(net, test_loader):\n",
    "    prec1_sum = 0\n",
    "    prec5_sum = 0\n",
    "    net.eval()\n",
    "    for i, (img, target) in enumerate(test_loader, start=1):\n",
    "        # print(f\"batch: {i}\")\n",
    "        img = img.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = net(img)\n",
    "        prec1, prec5 = accuracy(out, target, topk=(1, 5))\n",
    "        prec1_sum += prec1\n",
    "        prec5_sum += prec5\n",
    "        # print(f\"batch: {i}, acc1:{prec1}, acc5:{prec5}\")\n",
    "    print(f\"Acc_student:{prec1_sum / (i + 1)/100}, Acc_teacher: {prec5_sum / (i + 1)/100}\")\n",
    "\n",
    "\n",
    "def train(net_s, net_t, train_loader, test_loader):\n",
    "    # opt = Adam(filter(lambda p: p.requires_grad,net.parameters()), lr=0.0001)\n",
    "    opt = Adam(net_s.parameters(), lr=0.0001)\n",
    "    net_s.train()\n",
    "    net_t.eval()\n",
    "    for epoch in range(100):\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            opt.zero_grad()\n",
    "            image, target = batch\n",
    "            image = image.cuda()\n",
    "            target = target.cuda()\n",
    "            out_s, out_t = net_s(image), net_t(image)\n",
    "            loss_init = CrossEntropyLoss()(out_s, target)\n",
    "            loss_kd = KD_loss(T=4)(out_s, out_t)\n",
    "            loss = loss_init + loss_kd\n",
    "            # prec1, prec5 = accuracy(predict, target, topk=(1, 5))\n",
    "            # print(f\"epoch:{epoch}, step:{step}, loss:{loss.item()}, acc1: {prec1},acc5:{prec5}\")\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(f\"epoch:{epoch}, loss_init: {loss_init.item()}, loss_kd: {loss_kd.item()}, loss_all:{loss.item()}\")\n",
    "        test(net_s, test_loader)\n",
    "\n",
    "    torch.save(net_s.state_dict(), 'resnet9_cifar100_kd.pth')\n",
    "\n",
    "\n",
    "def main():\n",
    "    net_t = ResNet9(3, 100)\n",
    "    net_s = ResNet9_2(3, 100)\n",
    "    net_t = net_t.cuda()\n",
    "    net_s = net_s.cuda()\n",
    "    load_checkpoint(net_t, resnet50_pretrain_weight, exclude_fc=False)\n",
    "    load_checkpoint(net_s, resnet18_pretrain_weight, exclude_fc=True)\n",
    "    # for name, value in net.named_parameters():\n",
    "    #     if 'fc' not in name:\n",
    "    #         value.requires_grad = False\n",
    "\n",
    "  \n",
    "    train(net_s, net_t, trainloader, testloader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    current_time=time.time()\n",
    "    main()\n",
    "    time_train = time.time() - current_time\n",
    "    print('Training time: {:.2f} s'.format(time_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55b393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee869d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10304d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
